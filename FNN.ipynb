{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     \\nStudent Name:  ZHANG Xinyue                /        Qiao  Shuyu        /       Li Zuoxuan\\nStudent ID:    20750194                    /          20747563         /        20740917\\nStudent Email: xzhangfa@connect.ust.hk     /   sqiaoac@connect.ust.hk  /   zlify@connect.ust.hk\\nCourse Name:   MSBD5012\\nURL in github: https://github.com/orange-neng/MSBD5012-Forest-type-prediction-exploration\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"     \n",
    "Student Name:  ZHANG Xinyue                /        Qiao  Shuyu        /       Li Zhuoxuan\n",
    "Student ID:    20750194                    /          20747563         /        20740917\n",
    "Student Email: xzhangfa@connect.ust.hk     /   sqiaoac@connect.ust.hk  /   zlify@connect.ust.hk\n",
    "Course Name:   MSBD5012\n",
    "URL in github: https://github.com/orange-neng/MSBD5012-Forest-type-prediction-exploration\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "2. Classification\n",
    "After EDA, we will explore several kinds of classification\n",
    "methods and compare their accuracy.\n",
    "Different from traditional methods, we apply basic dense\n",
    "neural network (DNN) (as it's a numerical classification task)\n",
    "to see whether it can achieve a better performance then other\n",
    "methods.\n",
    "'''\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import Datasets\n",
    "# train = pd.read_csv('./cleaned_train.csv')\n",
    "# test = pd.read_csv('./cleaned_test.csv')\n",
    "# raw = pd.read_csv('./covtype.data')\n",
    "#\n",
    "# train = raw.iloc[:,1:-1]\n",
    "# target = raw.iloc[:, -1]\n",
    "#\n",
    "# clf = MLPClassifier(random_state=1,\n",
    "#                     max_iter=300,\n",
    "#                     hidden_layer_sizes=(30, 20),\n",
    "#                     activation='logistic')\n",
    "#\n",
    "# #3-fold cv\n",
    "# scores = cross_val_score(clf, train, target,\n",
    "#                          cv=3, n_jobs=-1, verbose=2)\n",
    "#\n",
    "#\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "'''\n",
    "ID = test.iloc[:,0]\n",
    "with open(\"./submission.csv\", \"w\") as subfile:\n",
    "    subfile.write(\"Id,Cover_Type\\n\")\n",
    "    for i in range(len(result)):\n",
    "        subfile.write(\"%s,%s\\n\"%(ID[i],result[i]))\n",
    "'''\n",
    "\n",
    "'''\n",
    "While using neural network, tuning will always be the hardest problem.\n",
    "We use the GirdSearchCV provided by sklearn for auto-tuning. \n",
    "As batch_size and epochs has a large effect to \n",
    "the running speed, for a better running performance, \n",
    "let's first check them.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learning_rate=0.001):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(225, input_dim=53, activation='relu'))\n",
    "    #\n",
    "    # model.add(Dense(120, activation=activation))\n",
    "    # model.add(Dense(7, activation='softmax'))\n",
    "    model.add(Dense(1024, input_dim=54, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataset = np.loadtxt(\"./covtype_normalize.csv\", delimiter=\",\", skiprows=1)\n",
    "# get the number of rows and columns\n",
    "r, c = dataset.shape\n",
    "np.random.shuffle(dataset)\n",
    "split = int(0.7*r)\n",
    "dataset[:, c-1] = dataset[:, c-1] - 1\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "x_train = np.asarray(dataset[:split, 1:c-1])\n",
    "y_train = np.asarray(dataset[:split, c-1])\n",
    "x_test = np.asarray(dataset[split:, 1:c-1])\n",
    "y_test = np.asarray(dataset[split:, c-1])\n",
    "\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1) #epochs=100, batch_size=500)\n",
    "# define the grid search parameters\n",
    "# activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "# dim = [20, 40, 60, 80, 100, 120, 120, 160, 180, 200]\n",
    "# param_grid = {'learning_rate': [0.001]}\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=1)\n",
    "\n",
    "# param_rand = {'learning_rate': np.linspace(0.01, 0.1, 10)}\n",
    "# param_rand = {'learning_rate': [0.001]}\n",
    "# grid = RandomizedSearchCV(model, param_rand, cv=3, n_jobs=-1)\n",
    "# grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size = 2048, validation_data = (x_test, y_test), use_multiprocessing=True)\n",
    "# print('training acc.:', history.history['acc'][-1],'\\n','test acc.:', (history.history['val_acc'])[-1])\n",
    "pd.DataFrame(history.history).to_csv(\"loghis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
