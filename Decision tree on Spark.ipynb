{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSBD 5012 group project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"     \n",
    "Student Name:  ZHANG Xinyue                /        Qiao  Shuyu        /       Li Zuoxuan\n",
    "Student ID:    20750194                    /          20747563         /        20740917\n",
    "Student Email: xzhangfa@connect.ust.hk     /   sqiaoac@connect.ust.hk  /   zlify@connect.ust.hk\n",
    "Course Name:   MSBD5012\n",
    "URL in github: https://github.com/orange-neng/MSBD5012-Forest-type-prediction-exploration\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "In this project, we predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (opposed to remotely sensed data). To finish the classification task, we first analyze the dataset to equip the future data pre-processing and application. Then we select multiple machine learning algorithms from various machine learning packages such as Sklearn, Keras and Spark MLlib , to compare their performance. In this report, the process of data analysis, data cleaning, data normalization and hyperparameter tuning will be described to show how they affect the final classification accuracy.\n",
    "* Here,this file applies Decesion tree form spark MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1.Load data & LabeledPoint object format\n",
    "**comment:** \n",
    "* Init returns all values except the last value; the last column is the target.\n",
    "* The decision tree requires label to start at 0, so subtract 1 from it.\n",
    "* convert to LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg._\n",
    "import org.apache.spark.mllib.regression._\n",
    "\n",
    "val rawData = sc.textFile(\"./forest-cover-type-prediction/covtype.data\")\n",
    "\n",
    "val data = rawData.map { line =>\n",
    "  val values = line.split(',').map(_.toDouble)\n",
    "  val featureVector = Vectors.dense(values.init)\n",
    "  val label = values.last - 1\n",
    "  LabeledPoint(label, featureVector)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.Data preprocessing\n",
    "**comment:** \n",
    "* We now divide the data into three complete parts: training set, cross-validation set (CV) and test set. \n",
    "* In the code below, you will see that the training set accounts for 80%, the cross-validation set and the test set each account for 10%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val Array(trainData, cvData, testData) =\n",
    "  data.randomSplit(Array(0.8, 0.1, 0.1))\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 3.Model\n",
    "**comment:** \n",
    "* Try to construct a DecisionTreeModel model on the training set, use default values for the parameters, and use the CV set to calculate the indicators of the resulting model:\n",
    "* trainClassifier indicates that the target in each LabeledPoint should be treated as a different class label, rather than a numerical feature value\n",
    "* The category label of the DecisionTreeModel model starts from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.evaluation._\n",
    "import org.apache.spark.mllib.tree._\n",
    "import org.apache.spark.mllib.tree.model._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "def getMetrics(model: DecisionTreeModel, data: RDD[LabeledPoint]):\n",
    "\n",
    "    MulticlassMetrics = {\n",
    "  val predictionsAndLabels = data.map(example =>\n",
    "    (model.predict(example.features), example.label)\n",
    "  )\n",
    "  new MulticlassMetrics(predictionsAndLabels)\n",
    "}\n",
    "\n",
    "val model = DecisionTree.trainClassifier(\n",
    "  trainData, 7, Map[Int,Int](), \"gini\", 4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val metrics = getMetrics(model, cvData)\n",
    "metrics.confusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics.precision _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(0 until 7).map(target => (metrics.precision(target), metrics.recall(target))).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.rdd._\n",
    "\n",
    "def classProbabilities(data: RDD[LabeledPoint]): Array[Double] = {\n",
    "  val countsByCategory = data.map(_.label).countByValue()\n",
    "  val counts = countsByCategory.toArray.sortBy(_._1).map(_._2)\n",
    "  counts.map(_.toDouble / counts.sum)\n",
    "}\n",
    "\n",
    "val trainPriorProbabilities = classProbabilities(trainData)\n",
    "val cvPriorProbabilities = classProbabilities(cvData)\n",
    "trainPriorProbabilities.zip(cvPriorProbabilities).map {\n",
    "  case (trainProb, cvProb) => trainProb * cvProb\n",
    "}.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DecisionTree.trainClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 4.Parameters tuning\n",
    "**comment:** \n",
    "* Triple for loop.\n",
    "* Sort and print in descending order according to the second value (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val evaluations =\n",
    "  for (impurity <- Array(\"gini\", \"entropy\");\n",
    "       depth    <- Array(1, 20);\n",
    "       bins     <- Array(10, 300))\n",
    "    yield {\n",
    "      val model = DecisionTree.trainClassifier(\n",
    "        trainData, 7, Map[Int,Int](), impurity, depth, bins)\n",
    "      val predictionsAndLabels = cvData.map(example =>\n",
    "        (model.predict(example.features), example.label)\n",
    "      )\n",
    "      val accuracy =\n",
    "        new MulticlassMetrics(predictionsAndLabels).precision(_)\n",
    "      ((impurity, depth, bins), accuracy)\n",
    "    }\n",
    "\n",
    "evaluations.sortBy(_._2).reverse.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 5.Evaluation\n",
    "**comment:** \n",
    "* The last step is to use the obtained hyperparameters to construct the model on the training set and CV set at the same time and evaluate it as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val model = DecisionTree.trainClassifier(\n",
    "  trainData.union(cvData), 7, Map[Int,Int](), \"entropy\", 20, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
